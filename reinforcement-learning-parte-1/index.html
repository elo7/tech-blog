<!DOCTYPE html>
<html>
    <head>
	<meta charset='UTF-8'>
	<meta name='viewport' content='width=device-width, initial-scale=1'>
	<meta name='description' content='Blog de tecnologia do Elo7, mantido pelo nosso time de Engenharia, compartilhando conhecimento e mostrando como é o dia a dia de um colaborador fora de série.'>
	<meta name='google-site-verification' content='NqCILBTY8B8P-r_KF8BSZKH9kUQgQOEbXJvEMaB33vw'>
	<meta name='google-site-verification' content='cKh-stJM3_ENNfMjaBIIyYiDgMXZFpRkoH8eQTcPwhM' />
	<meta name="google-site-verification" content="6Er6NBhORIKrEwdS46h772O_7LTE9vWgZjwtzuZ5gpQ" />
	<meta name='theme-color' content='#FDC24F'>
	<meta name='keywords' content='Elo7,tecnologia,post,desenvolvimento,blog,reinforcement,learning,machine,artificial,intelligence,'>
	<meta name='language' content='pt-br'>
	<meta name='title' content='Elo7 Tech - Reinforcement Learning Parte 1 - Introdução'>
	<meta name='apple-mobile-web-app-title' content='Elo7 Tech - Reinforcement Learning Parte 1 - Introdução'>
	<meta name='mobile-web-app-capable' content='yes'>

	<meta property='fb:app_id' content='644444999041914'>
	<meta property='fb:admins' content='100003324447975'>

	<meta property='og:site_name' content='Elo7 Tech'>
	<meta property='og:image' content='https://elo7.dev/images/cover/reinforcement-learning-parte-1.png'>
	<meta property='og:type' content='website'>
	<meta property='og:title' content='Elo7 Tech - Reinforcement Learning Parte 1 - Introdução'>
	<meta property='og:url' content='https://elo7.dev/reinforcement-learning-parte-1/'>
	<meta property='og:description' content='Blog de tecnologia do Elo7, mantido pelo nosso time de Engenharia, compartilhando conhecimento e mostrando como é o dia a dia de um colaborador fora de série.'>

	<meta name='twitter:widgets:csp' content='on'>
	<meta name='twitter:card' content='summary_large_image'>

	<meta property='twitter:title' content='Elo7 Tech - Reinforcement Learning Parte 1 - Introdução'>
	<meta property='twitter:domain' content='https://elo7.dev/'>
	<meta property='twitter:url' content='https://elo7.dev/reinforcement-learning-parte-1/'>
	<meta property='twitter:description' content='Blog de tecnologia do Elo7, mantido pelo nosso time de Engenharia, compartilhando conhecimento e mostrando como é o dia a dia de um colaborador fora de série.'>
	<meta property='twitter:image' content='https://elo7.dev/images/cover/reinforcement-learning-parte-1.png'>

	<link rel='canonical' href='https://elo7.dev/reinforcement-learning-parte-1/'>
	<title>Elo7 Tech - Reinforcement Learning Parte 1 - Introdução</title>
	<link rel='stylesheet' href='https://elo7.dev/css/reset.css'>
	<link rel='stylesheet' href='https://elo7.dev/css/vendor/highlight.css' >
	<link rel='stylesheet' href='https://elo7.dev/css/main.css' >
	<link rel='stylesheet' href='https://elo7.dev/css/posts.css' >
	<link rel='stylesheet' href='https://elo7.dev/css/post.css' >
	<link rel='stylesheet' href='https://elo7.dev/css/publisher.css' >
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-16.png' sizes='16x16'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-32.png' sizes='32x32'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-48.png' sizes='48x48'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-64.png' sizes='64x64'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-96.png' sizes='96x96'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-128.png' sizes='128x128'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-160.png' sizes='160x160'>
	<link rel='icon' href='https://elo7.dev/images/favicon/favicon-192.png' sizes='192x192'>
	<link rel='apple-touch-icon-precomposed' sizes='180x180' href='https://elo7.dev/images/favicon/favicon-180.png'>
	<link rel='apple-touch-icon-precomposed' sizes='152x152' href='https://elo7.dev/images/favicon/favicon-152.png'>
	<link rel='apple-touch-icon-precomposed' sizes='144x144' href='https://elo7.dev/images/favicon/favicon-144.png'>
	<link rel='apple-touch-icon-precomposed' sizes='120x120' href='https://elo7.dev/images/favicon/favicon-120.png'>
	<link rel='apple-touch-icon-precomposed' sizes='114x114' href='https://elo7.dev/images/favicon/favicon-114.png'>
	<link rel='apple-touch-icon-precomposed' sizes='76x76' href='https://elo7.dev/images/favicon/favicon-76.png'>
	<link rel='apple-touch-icon-precomposed' sizes='72x72' href='https://elo7.dev/images/favicon/favicon-72.png'>
	<link rel='apple-touch-icon-precomposed' sizes='60x60' href='https://elo7.dev/images/favicon/favicon-60.png'>
	<link rel='apple-touch-icon-precomposed' sizes='57x57' href='https://elo7.dev/images/favicon/favicon-57.png'>
	<link rel='apple-touch-icon-precomposed' href='https://elo7.dev/images/favicon/favicon-precomposed.png'>
	<script>window.addEventListener('error', window.__e=function f(e){f.q=f.q||[];f.q.push(e)});</script>
	<script src='/js/vendor/async-define.js'></script>
</head>

    <body>
        <header class='left-pane'>
    <div class='logo-container'>
        <a rel='home' itemprop='url' href='https://elo7.dev/' class='logo'>Tech Blog Elo7</a>
    </div>
    <div class='navigation'>
        <input id='categories-switch' type='checkbox' class='categories-switch'>
        <label for='categories-switch' class='selectable'>
            <h2 class='nav-title'>Categorias</h2>
        </label>
        <nav aria-label='Navegue pelas categorias do nosso blog' class='nav-list nav-category'>
            
                <a href='/categoria/back-end' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Back End</a></span>
                </a>
            
                <a href='/categoria/big-data' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Big Data</a></span>
                </a>
            
                <a href='/categoria/cultura' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Cultura</a></span>
                </a>
            
                <a href='/categoria/design' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Design</a></span>
                </a>
            
                <a href='/categoria/devops' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Devops</a></span>
                </a>
            
                <a href='/categoria/eventos' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Eventos</a></span>
                </a>
            
                <a href='/categoria/front-end' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Front End</a></span>
                </a>
            
                <a href='/categoria/machine-learning' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Machine Learning</a></span>
                </a>
            
                <a href='/categoria/mobile' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Mobile</a></span>
                </a>
            
                <a href='/categoria/vagas' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                    <span itemprop='name'>Vagas</a></span>
                </a>
            
            <a href='/talks' itemscope itemtype='http://schema.org/SiteNavigationElement'>
                <span itemprop='name'>Palestras</span>
            </a>
        </nav>
    </div>
    <div class='navigation'>
        <input id='more-switch' type='checkbox' class='more-switch'>
        <label for='more-switch' class='selectable'>
            <h2 class='nav-title'>Veja também</h2>
        </label>
        <nav class='nav-list nav-more' aria-label='Navegue pelos links relacionados ao Elo7'>
            <a itemprop='relatedLink' href='/meetups'>
                <span itemprop='name'>Meetups no Elo7</span>
            </a>
            <a itemprop='relatedLink' href='http://carreira.elo7.com.br/engenharia/' target='_blank'>
                <span itemprop='name'>A engenharia</span>
            </a>
            <a itemprop='relatedLink' href='http://carreira.elo7.com.br/' target='_blank'>
                <span itemprop='name'>Carreiras</span>
            </a>
            <a itemprop='relatedLink' href='http://elo7.com.br/' target='_blank'>
                <span itemprop='name'>Elo7</span>
            </a>
        </nav>
    </div>
    <div class='social'>
        <a title='Github do Elo7' rel='external' itemprop='relatedLink' href='https://github.com/elo7' target='_blank' class='github'>Github do Elo7</a>
        <a title='Twitter do Elo7' rel='external' itemprop='relatedLink' href='https://twitter.com/elo7tech' target='_blank' class='twitter'>Twitter do Elo7</a>
        <a title='RSS do Elo7' rel='external' itemprop='relatedLink' href='https://elo7.dev/index.xml' target='_blank' class='rss'>RSS do Elo7</a>
        <a title='Newsletter do Elo7' rel='external' itemprop='relatedLink' href='http://eepurl.com/cVUwvH' target='_blank' class='email'>Newsletter do Elo7</a>
    </div>
</header>

        <main aria-label='Conteúdo principal' itemscope itemtype='http://schema.org/Blog'>
            <article itemprop='blogPost' itemscope itemtype='http://schema.org/BlogPosting' class='post-main'>
                <figure class='cover-image' itemprop="image" itemscope itemtype="http://schema.org/ImageObject">
                    <img src='/images/cover/reinforcement-learning-parte-1.png' alt='Reinforcement Learning Parte 1 - Introdução' itemprop="url">
                </figure>
                <div class='post-content'>
                    <h1 itemprop='name' class='title'>Reinforcement Learning Parte 1 - Introdução</h1>
                    <div class='post-meta'>
                        <p class='date'>
                            Publicado em:
                            <time class='date'
    datetime='2019-06-17 00:00:00 &#43;0000 UTC'
    aria-label='17 de junho de 2019'>
    17/06/2019
    <meta itemprop='datePublished' content='Mon Jun 17 2019 00:00:00 GMT&#43;0000 (UTC)'/>
    <meta itemprop='dateModified' content='Mon Jun 17 2019 00:00:00 GMT&#43;0000 (UTC)'>
    <meta name='date' content='2019-06-17'>
</time>

                        </p>

                        <article>
                            
                                <a data-author='onimaru' itemprop='author' itemscope itemtype='http://schema.org/Person' rel='author' href='/autor/onimaru/' class='author'>
                                    <meta name='author' itemprop='url' content='/autor/onimaru'>
                                    <img class='hide avatar' width='50px' height='50px' itemprop='image'>
                                    <p itemprop='name' class='publisher' data-author='onimaru'>@onimaru</p>
                                </a>
                            

                            <meta itemprop='worksFor' content='Elo7 Serviços de Informática SA'>
                        </article>
                    </div>
                    <div itemprop='articleBody'>
                        

<p>Esta é a primeira parte de uma série de posts nos quais vamos concentrar nossa atenção em uma parte de Machine Learning (<strong><em>ML</em></strong>) chamada <strong>Aprendizado por Reforço</strong>. Esta talvez seja a parte menos conhecida de ML e é aquela que possui maior semelhança com os métodos que humanos e animais usam para aprender a realizar tarefas, e ela inicialmente também envolve uma menor abstração matemática para entender o funcionamento dos algoritmos.</p>

<p>A intenção desta série é apresentar e explicar desde os princípios básicos até a demonstração do funcionamento dos principais algoritmos. Para isso, vamos utilizar um pouco de matemática (tentarei explicar o melhor possível, prometo!) e Python. Se você procurar em vários lugares (como o <a href="medium.com">Medium</a>) verá que quase sempre há referência ao livro <strong><em>Reinforcement Learning An Introduction</em></strong>, dos autores <em>Richard S. Sutton</em> e <em>Andrew G. Barto</em>, pois é o livro mais influente nessa área e, na minha opinião o melhor lugar para se começar. Os dois autores possuem uma longa carreira pesquisando sobre Aprendizado por Reforço e ao longo do livro apresentam bem todas as características dos artigos e propõem vários pequenos problemas a serem resolvidos. Essa obra será também nossa principal referência.</p>

<p>Devido à forte influência do livro a <a href="https://openai.com/">OpenAi</a> criou o <a href="https://gym.openai.com/">Gym</a>, um <em>toolkit</em> de ambientes (em Python) baseados nos problemas sugeridos no livro. Esses ambientes (<code>environments</code>) simulam as situações propostas para que pesquisadores, estudantes e curiosos possam testar e comparar seus algoritmos sem se preocupar com o ambiente. Portanto, vamos nos aproveitar da facilidade e boa parte das aplicações que usaremos utilizará esses ambientes.</p>

<h2 id="o-que-é-aprendizado-por-reforço">O que é Aprendizado por Reforço?</h2>

<p>Em <em>Machine Learning</em>, os tipos de abordagens de problemas se dividem em três classes: <strong>Aprendizado Supervisionado (Supervised Learning)</strong>; <strong>Aprendizado Não-Supervisionado (Unsupervised Learning)</strong>; e <strong>Aprendizado por Reforço (Reinforcement Learning ou RL)</strong>. Para ilustrar, usamos uma imagem retirada <a href="http://www.cognub.com/index.php/cognitive-platform/">daqui</a>:</p>

<p><img src="/images/reinforcement-learning-parte-1-introducao-1.png" alt="Maze01" /></p>

<p>Em <em>RL</em>, podemos sempre imaginar que temos um robô dentro de um jogo ou mundo. O objetivo dele é aprender como vencer o jogo de maneira ótima (mais pontos, menor tempo, com mais vidas sobrando, etc). Para indicar como o robô está se saindo durante as jogadas ele precisa de algum tipo de indicação sobre seu comportamento, ou seja, existe uma espécie de pontuação recebida como recompensa quando uma ação boa é executada. Damos os nomes de <strong><em>agente (agent)</em></strong> ao robô e <strong><em>ambiente (environment)</em></strong> ao jogo dentro do qual ele se encontra.</p>

<p>Bom, vamos imaginar um jogo bem simples: nosso agente (O) está dentro de um labirinto que contém armadilhas (T) e a saída (X):</p>

<p><img src="/images/reinforcement-learning-parte-1-introducao-2.png" alt="Maze02" /></p>

<p>As regras são as seguintes:</p>

<ul>
<li>o agente inicia em um quadrado aleatório desde que não seja a saída ou uma das armadilhas;</li>
<li>o movimento possível é andar para um quadrado adjacente ao que ele está, então na imagem acima ele não pode andar para a esquerda;</li>
<li>se o movimento resultar em uma posição que não é armadilha ou saída, o agente pode se mover mais uma vez;</li>
<li>se o movimento levá-lo a uma armadilha, o agente perde a partida (perde uma vida) e deve iniciar novamente;</li>
<li>se o movimento levá-lo ao quadrado da saída, ele vence a partida.</li>
</ul>

<p>Se você já viu algo sobre esse assunto deve ter se deparado com vários termos como &ldquo;agente&rdquo;, &ldquo;retorno&rdquo;, &ldquo;recompensa&rdquo;, &ldquo;estado&rdquo;, entre outros. Para nos acostumarmos com o que virá, vamos deixar claro desde já os principais termos que usaremos por aqui.</p>

<h3 id="action-a">Action (A)</h3>

<p><strong>Action</strong> (<strong>ação</strong>) é o conjunto de movimentos ou jogadas que o agente pode executar. Uma ação qualquer dentro das possíveis é usualmente denotada como $a \in A$. Ex: no labirinto, o agente pode ter no máximo 4 ações diferentes: andar para cima, baixo, esquerda ou direita. Em alguns momentos as quatro ações podem estar disponíveis, em outros, apenas três ou duas.</p>

<h3 id="state-s">State (S)</h3>

<p><strong>State</strong> (<strong>estado</strong>) é a situação na qual o agente está dentro do ambiente. Um estado qualquer dentro dos possíveis é usualmente denotado como $s \in S$. Ex: no labirinto o estado é simplesmente a posição ou o quadrado ocupado pelo agente. Os quadrados com armadilhas ou saída são chamados de <em>estados terminais (terminate states)</em>, pois causam o término da partida.</p>

<h3 id="reward-r">Reward &reg;</h3>

<p><strong>Reward</strong> (<strong>recompensa</strong>) é uma quantidade numérica fornecida ao agente quando este executa ações que o levam a vencer o jogo ou são apenas consideradas boas. A recompensa é dada na maioria dos casos por ações executadas recentemente, geralmente a última. Ex: no nosso problema nós definimos quando o robô ganha uma recompensa, pode ser apenas quando vence uma partida ou até mesmo quando se aproxima dela. Funciona como uma espécie de medida de quão próximo o agente está de cumprir o objetivo.</p>

<h3 id="policy-pi">Policy ($\pi$)</h3>

<p><strong>Policy</strong> (<strong>política</strong>) é a estratégia empregada pelo agente para escolher as ações baseado no estado em que ele se encontra. Ela é uma parte importante da modelagem do problema. Em muitos casos, não há nenhum tipo de estratégia clara e se costuma utilizar uma <em>policy</em> aleatória. Ex: além do movimento aleatório, poderíamos informar ao agente algo como &ldquo;se o movimento para a cima não é permitido, ande para baixo&rdquo;, ou mesmo: &ldquo;na dúvida, use um movimento que irá para a direita ou para baixo com a mesma probabilidade&rdquo;.</p>

<h3 id="value-v">Value (V)</h3>

<p><strong>Value</strong> (<strong>valor</strong>), também chamado de <strong>state-value</strong>, é definido como o valor esperado do retorno (uma espécie de recompensa) a longo prazo. Basicamente, isso seria o valor médio de recompensa ganha no fim da partida associado a um determinado estado. Ex: no labirinto, um quadrado ao lado da saída e longe de armadilhas tem uma chance maior de levar o robô a vencer a partida do que um quadrado ao lado de uma armadilha e longe da saída. Assim, se o valor dos estados for conhecido, o robô pode optar por ir para o estado que tem maior valor.</p>

<h3 id="q-value-q">Q-value (Q)</h3>

<p>Também conhecido como <strong>action-state-value</strong>, é similar ao <em>state-value</em>, porém associado à recompensa de curto prazo. Dado um estado que disponibiliza mais de uma ação possível, aquelas que levam o agente a vencer a partida têm um valor maior que as demais. Ex: na imagem abaixo, o agente está em um estado ($s_{3,4} = s_{linha\ 3, coluna\ 4}$) que permite as ações $a_{cima}$, $a_{esquerda}$ e $a_{baixo}$. Vemos que a última leva à saída e a primeira a um estado próximo de perder, portanto, os valores de cada ação possivelmente respeitariam a seguinte relação: $Q(s_{3,4},a_{baixo}) &gt; Q(s_{3,4},a_{esquerda}) &gt; Q(s_{3,4},a_{cima})$.</p>

<p><img src="/images/reinforcement-learning-parte-1-introducao-3.png" alt="Maze03" /></p>

<h3 id="step-e-episode">Step e Episode</h3>

<p><strong>Step</strong> determina uma passagem de tempo; usualmente, cada vez que o agente executa uma ação se passa um <em>step</em>. Ex: podemos considerar o <em>step</em> no labirinto quase que literalmente como um passo, pois quando o agente &ldquo;anda&rdquo; para um quadrado adjacente ele usa um <em>step</em>.</p>

<p><strong>Episode</strong> é uma partida completa. Pode durar um número fixo de <em>steps</em> ou ser variável. Ex: sempre que o estado é igual a uma armadilha ou saída, um episódio é encerrado.</p>

<h2 id="conclusão">Conclusão</h2>

<p>Conhecendo um pouco da nomenclatura já podemos ter uma ideia do que acontece em RL. Se nos foi dada a tarefa de criar um agente que é capaz de sair deste labirinto, nosso verdadeiro objetivo é utilizar algum método (veremos quais) que é capaz de <strong><em>aprender qual é a optimal policy</em></strong>, ou seja, qual é a estratégia que fará o agente vencer o maior número possível de partidas, não importando em que estado ele inicie.</p>

<p>Para problemas simples como o nosso, existe tal estratégia (pode existir mais de uma) e nós, humanos, conseguimos encontrá-la facilmente, como mostramos na figura abaixo. Já ensinar um robô pode não ser tão simples.</p>

<p><img src="/images/reinforcement-learning-parte-1-introducao-4.png" alt="Maze04" /></p>

<p>Para problemas mais complexos, como sair de um labirinto dinâmico, jogar xadrez, dirigir um carro ou jogar em um cassino não há tal garantia e, portanto, procuramos uma <em>policy</em> que seja aproximadamente ótima. A maneira que os algoritmos utilizam para resolver os problemas gira em torno de técnicas para estimar <em>state-values</em> e <em>action-state-values</em> através da experiência adquirida pelo agente ao longo dos episódios.</p>

<p>Nossa missão nos próximos posts é utilizar essas técnicas para resolver vários problemas. No próximo post, vamos justamente abordar a técnica conhecida como <strong>Q-Learning</strong> para ensinar nosso robô a fugir de um labirinto parecido com o que vimos, mas que possuirá um risco extra, pois o agente andará sobre o gelo.</p>


                        <ul class='tag-list'>
                            
                                <li>
                                    <a href='/tags/reinforcement/'>reinforcement</a>
                                </li>
                            
                                <li>
                                    <a href='/tags/learning/'>learning</a>
                                </li>
                            
                                <li>
                                    <a href='/tags/machine/'>machine</a>
                                </li>
                            
                                <li>
                                    <a href='/tags/artificial/'>artificial</a>
                                </li>
                            
                                <li>
                                    <a href='/tags/intelligence/'>intelligence</a>
                                </li>
                            
                        </ul>
                        <section class='share'>
                            <a href='#share' class='share-post hide' title='Clique aqui para compartilhar esse post'>Compartilhe</a>
                            <div class='social-share'>
                                <a href='https://www.facebook.com/dialog/share?app_id=644444999041914&href=https%3a%2f%2felo7.dev%2freinforcement-learning-parte-1%2f&display=popup' rel='noopener' target='_blank' class='link-share facebook' title='Clique para compartilhar no Facebook'>
                                    Compartilhar no facebook
                                </a>
                                <a href='https://twitter.com/intent/tweet?text=reinforcement-learning-parte-1-introdu%25C3%25A7%25C3%25A3o&url=https%3a%2f%2felo7.dev%2freinforcement-learning-parte-1%2f&hashtags=elo7tech' rel='noopener' target='_blank' class='link-share twitter' title='Clique para compartilhar no Twitter'>
                                    Compartilhar no twitter
                                </a>
                                <a href='https://elo7.dev/reinforcement-learning-parte-1/?utm_source=share&utm_medium=copy' class='link-share hide copy' title='Clique para copiar a url'>
                                    Copiar URL
                                </a>
                                <span class='copy-success'>Link copiado</span>
                                <input type='url' value='https://elo7.dev/reinforcement-learning-parte-1/?utm_source=share&utm_medium=copy' class='link-input'>
                            </div>
                        </section>
                    </div>
                    <meta itemprop='headline' content='Vamos iniciar o estudo sobre Reinforcement Learning abordando os termos mais comuns utilizados nesta área enquanto tentamos ensinar nosso agente a não cair em armadilhas.'/>
                    <span itemprop='publisher' itemscope itemtype='http://schema.org/Organization'>
                        <meta itemprop='name' content='Elo7 Tech'/>
                        <meta itemprop='url' content='https://elo7.dev/'/>
                        <span itemprop='logo' itemscope itemtype='http://schema.org/ImageObject'>
                            <link href='https://images.elo7.com.br/assets/v3/desktop/png/logo-elo7.png' itemprop='url'/>
                            <meta itemprop='width' content='100px'/>
                            <meta itemprop='height' content='100px'/>
                        </span>
                    </span>
                    <meta itemprop='mainEntityOfPage' content='Elo7 Serviços de Informática SA'/>

                    <div id='disqus_thread'></div>
                </div>
            </article>
        </main>

        <footer itemscope itemtype='http://schema.org/Organization'>
    <a rel='home' itemprop='url' href='https://elo7.dev/' >
        elo7.dev © <script>now = new Date; document.write(now.getFullYear());</script>
    </a>
    <meta itemprop='name' content='Elo7 Serviços de Informática SA'/>
    <section class='footer-social'>
        <a title='Github do Elo7' rel='external' itemprop='url' href='https://github.com/elo7' target='_blank' class='github'>Github do Elo7</a>
        <a title='Twitter do Elo7' rel='external' itemprop='url' href='https://twitter.com/elo7tech' target='_blank' class='twitter'>Twitter do Elo7</a>
        <a title='RSS do Elo7' rel='external' itemprop='url' href='https://elo7.dev/index.xml' target='_blank' class='rss'>RSS do Elo7</a>
        <a title='Newsletter do Elo7' rel='external' itemprop='url' href='http://eepurl.com/cVUwvH' target='_blank' class='email'>Newsletter do Elo7</a>
    </section>
</footer>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<script async src='/js/analytics.js'></script>
<script async src='/js/github.js'></script>
<script async src='/js/vendor/events-amd.js'></script>
<script async src='/js/vendor/ajax.js'></script>
<script async src='/js/vendor/doc.js'></script>
<script type='text/x-mathjax-config'>
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ],
            displayMath: [ ['$$','$$'], ['\[','\]'] ],
            processEscapes: true
        },
    });
</script>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script src='/js/vendor/highlight.js'></script>
<script>hljs.initHighlightingOnLoad();</script>


        <script>
            var disqus_shortname = 'engenhariaelo7';
            var disqus_identifier = '2019-06-17 00:00:00 \x2b0000 UTC:\/reinforcement-learning-parte-1\/';
            var disqus_url = 'https:\/\/elo7.dev\/reinforcement-learning-parte-1\/';

            (function() {
                var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Habilite o JavaScript para ver os comentários</noscript>
        <script async src='/js/post.js'></script>
    </body>
</html>
